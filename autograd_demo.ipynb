{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd Demo\n",
    "\n",
    "This file demonstates how to:\n",
    "\n",
    "* Use the **autograd** Python package to compute gradients of functions\n",
    "* Use gradients from autograd to do a basic linear regression\n",
    "\n",
    "# Takeaways\n",
    "\n",
    "* Automatic differentiation is a powerful idea that has made experimenting with different models and loss functions far easier than it was even 8 years ago.\n",
    "* The Python package `autograd` is a wonderfully simple tool that makes this work with numpy/scipy\n",
    "\n",
    "* `autograd` works by a super-smartly implemented version of the backpropagation dynamic programming we've already discussed from Unit 3\n",
    "    * Basically, after doing a \"forward\" pass to evaluate the function, we do a \"reverse\" pass through the computation graph and compute gradients via the chain rule.\n",
    "    * This general purpose method is called [reverse-mode differentiation](https://github.com/HIPS/autograd/blob/master/docs/tutorial.md#reverse-mode-differentiation)\n",
    "\n",
    "* `autograd` does NOT do symbolic math!\n",
    "    * e.g. It does not simplify `ag_np.sqrt(ag_np.square(x))` as `x`. It will use the chain rule on all nested functions that the user specifies.\n",
    "* `autograd` does NOT do numerical approximations to gradients.\n",
    "    * e.g. It does not estimate gradients by perturbing inputs slightly\n",
    "\n",
    "* We'll see how we can define losses in terms of dictionaries, which let us define complicated models with many different parameters. This code specifically is what you'll want to in Project C for matrix factorization with many parameters.\n",
    "\n",
    "# Limitations\n",
    "\n",
    "FYI There are some things that autograd *cannot* handle that you should be aware of. \n",
    "\n",
    "Make sure any loss function you define that you want to differentiate does not do any of these things:\n",
    "\n",
    "* Do not use assignment to elements of arrays, like `A[0] = x` or `A[1] = y`\n",
    "    * Instead, compute entries individually and then stack them together.\n",
    "    * Like this: `x = ...; y = ...; A = ag_np.hstack([x, y])`\n",
    "* Do not rely on implicit casting of lists to arrays, like `A = ag_np.sum([x, y])`\n",
    "    * use `A = ag_np.sum(ag_np.array([x, y]))` instead.\n",
    "* Do not use A.dot(B) notation\n",
    "    * Instead, use `ag_np.dot(A, B)`\n",
    "* Avoid in-place operations (such as `a += b`)\n",
    "    * Instead, use a = a + b\n",
    "\n",
    "# Further Reading\n",
    "\n",
    "Check out these great resources\n",
    "\n",
    "* Official tutorial for the autograd package: https://github.com/HIPS/autograd/blob/master/docs/tutorial.md\n",
    "* Short list of what autograd *can* and *cannot* do: https://github.com/HIPS/autograd/blob/master/docs/tutorial.md#supported-and-unsupported-parts-of-numpyscipy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import autograd\n",
    "import autograd.numpy as ag_np\n",
    "import autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn') # pretty matplotlib plots\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"part1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: Using autograd.grad for univariate functions\n",
    "\n",
    "Suppose we have a mathematical function of interest $f(x)$.\n",
    "\n",
    "For now, we'll assume this function has a scalar input and scalar output. This means:\n",
    "\n",
    "* $x \\in \\mathbb{R}$\n",
    "* $f(x) \\in \\mathbb{R}$\n",
    "\n",
    "We can ask: what is the derivative (aka *gradient*) of this function:\n",
    "\n",
    "$$\n",
    "g(x) \\triangleq \\frac{\\partial}{\\partial x} f(x)\n",
    "$$\n",
    "\n",
    "Instead of computing this gradient by hand via calculus/algebra, we can use `autograd` to do it for us.\n",
    "\n",
    "First, we need to implement the math function $f(x)$ as a **Python function** `f`.\n",
    "\n",
    "The Python function `f` needs to satisfy the following requirements:\n",
    "* INPUT 'x': scalar float\n",
    "* OUTPUT 'f(x)': scalar float\n",
    "* All internal operations are composed of calls to functions from `ag_np`, the `autograd` version of numpy\n",
    "\n",
    "### From numpy to autograd's wrapper of numpy\n",
    "\n",
    "You might be used to importing numpy as `import numpy as np`, and then using this shorthand for `np.cos(0.0)` or `np.square(5.0)` etc.\n",
    "\n",
    "For autograd to work, you need to instead use **autograd's** provided numpy wrapper interface:\n",
    "\n",
    "`from autograd.numpy as ag_np`\n",
    "\n",
    "The `ag_np` module has the same API as `numpy`. So for example, you can call\n",
    "\n",
    "* `ag_np.cos(0.0)`\n",
    "* `ag_np.square(5.0)`\n",
    "* `ag_np.sum(a_N)`\n",
    "* `ag_np.mean(a_N)`\n",
    "* `ag_np.dot(u_NK, v_KM)`\n",
    "\n",
    "Or almost any other function you usually would use with `np`\n",
    "\n",
    "**Summary:** Make sure your function `f` produces a scalar and only uses functions within the `ag_np` wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: f(x) = x^2\n",
    "\n",
    "$$\n",
    "f(x) = x^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return ag_np.square(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing gradients with autograd\n",
    "\n",
    "Given a Python function `f` that meets our requirements and evaluates $f(x)$, we want a Python function ``g` that computes the gradient $g(x) \\triangleq \\frac{\\partial}{\\partial x}$\n",
    "\n",
    "We can use `autograd.grad` to create a Python function `g` \n",
    "\n",
    "```\n",
    "g = autograd.grad(f) # create function g that produces gradients of input function f\n",
    "```\n",
    "\n",
    "The symbol `g` is now a **Python function** that takes the same input as `f`, but produces the derivative at a given input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = autograd.grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'g' is just a function.\n",
    "# You can call it as usual, by providing a possible scalar float input\n",
    "\n",
    "g(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot to demonstrate the gradient function  side-by-side with original function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input values evenly spaced between -5 and 5\n",
    "x_grid_G = np.linspace(-5, 5, 100)\n",
    "\n",
    "fig_h, subplot_grid = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True, squeeze=False)\n",
    "subplot_grid[0,0].plot(x_grid_G, [f(x_g) for x_g in x_grid_G], 'k.-')\n",
    "subplot_grid[0,0].set_title('f(x) = x^2')\n",
    "\n",
    "subplot_grid[0,1].plot(x_grid_G, [g(x_g) for x_g in x_grid_G], 'b.-')\n",
    "subplot_grid[0,1].set_title('gradient of f(x)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: Using autograd.grad for functions with multivariate input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine the input $x$ could be a vector of size D. \n",
    "\n",
    "Our mathematical function $f(x)$ will map each input vector to a scalar.\n",
    "\n",
    "We want the gradient function\n",
    "\n",
    "\\begin{align}\n",
    "g(x) &\\triangleq \\nabla_x f(x)\n",
    "\\\\\n",
    "&= [\n",
    "    \\frac{\\partial}{\\partial x_1} f(x)\n",
    "    \\quad \\frac{\\partial}{\\partial x_2} f(x)\n",
    "    \\quad \\ldots \\quad \\frac{\\partial}{\\partial x_D} f(x)  ]\n",
    "\\end{align}\n",
    "\n",
    "Instead of computing this gradient by hand via calculus/algebra, we can use autograd to do it for us.\n",
    "\n",
    "First, we implement math function $f(x)$ as a **Python function** `f`.\n",
    "\n",
    "The Python function `f` needs to satisfy the following requirements:\n",
    "* INPUT 'x': numpy array of float\n",
    "* OUTPUT 'f(x)': scalar float\n",
    "* All internal operations are composed of calls to functions from `ag_np`, the `autograd` version of numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worked Example 2a\n",
    "\n",
    "Let's set up a function that is defined as the inner product of the input vector x with some weights $w$\n",
    "\n",
    "We assume both $x$ and $w$ are $D$ dimensional vectors\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{d=1}^D x_d w_d\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the fixed weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2\n",
    "\n",
    "w_D = np.asarray([1., 2.,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function `f` using `ag_np` wrapper functions only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x_D):\n",
    "    return ag_np.dot(x_D, w_D) # dot product is just inner product in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `autograd.grad` to get the gradient function `g`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = autograd.grad(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try putting in the all-zero vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_D = np.zeros(D)\n",
    "\n",
    "print(\"x_D\", x_D)\n",
    "print(\"f(x_D) = %.3f\" % (f(x_D)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the gradient wrt that all-zero vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(x_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try another input vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_D = np.asarray([1., 2.])\n",
    "\n",
    "print(\"x_D\", x_D)\n",
    "print(\"f(x_D) = %.3f\" % (f(x_D)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the gradient wrt the vector [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(x_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Using autograd gradients within gradient descent to solve multivariate optimization problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function: basic gradient descent\n",
    "\n",
    "Here's a very simple function that will perform many gradient descent steps to optimize a given function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_many_iters_of_gradient_descent(f, g, init_x_D=None, n_iters=100, step_size=0.001):\n",
    "    ''' Run many iterations of GD\n",
    "    \n",
    "    Args\n",
    "    ---- \n",
    "    f : python function (D,) to float\n",
    "        Maps vector x_D to scalar loss\n",
    "    g : python function, (D,) to (D,)\n",
    "        Maps vector x_D to gradient g_D\n",
    "    init_x_D : 1D array, shape (D,)\n",
    "        Initial value for the input vector\n",
    "    n_iters : int\n",
    "        Number of gradient descent update steps to perform\n",
    "    step_size : positive float\n",
    "        Step size or learning rate for GD\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x_D : 1D array, shape (D,)\n",
    "        Best value of input vector for provided loss f found via this GD procedure\n",
    "    history : dict\n",
    "        Contains history of this GD run useful for plotting diagnostics\n",
    "    '''\n",
    "    # Copy the initial parameter vector\n",
    "    x_D = copy.deepcopy(init_x_D)\n",
    "\n",
    "    # Create data structs to track the per-iteration history of different quantities\n",
    "    history = dict(\n",
    "        iter=[],\n",
    "        f=[],\n",
    "        x_D=[],\n",
    "        g_D=[])\n",
    "\n",
    "    for iter_id in range(n_iters):\n",
    "        if iter_id > 0:\n",
    "            x_D = x_D - step_size * g(x_D)\n",
    "\n",
    "        history['iter'].append(iter_id)\n",
    "        history['f'].append(f(x_D))\n",
    "        history['x_D'].append(x_D)\n",
    "        history['g_D'].append(g(x_D))\n",
    "    return x_D, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worked Example 3a: Minimize f(x) = sum(square(x))\n",
    "\n",
    "It's easy to figure out that the vector with smallest L2 norm (smallest sum of squares) is the all-zero vector.\n",
    "\n",
    "Here's a quick example of showing that using gradient functions provided by autograd can help us solve the optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_x  \\sum_{d=1}^D x_d^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x_D):\n",
    "    return ag_np.sum(ag_np.square(x_D))\n",
    "\n",
    "g = autograd.grad(f)\n",
    "\n",
    "# Initialize at x_D = [6, 4, -3, -5]\n",
    "D = 4\n",
    "init_x_D = np.asarray([6.0, 4.0, -3.0, -5.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_x_D, history = run_many_iters_of_gradient_descent(f, g, init_x_D, n_iters=1000, step_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plots of how x parameter values evolve over iterations, and function values evolve over iterations\n",
    "# Expected result: f goes to zero. all x values goto zero.\n",
    "\n",
    "fig_h, subplot_grid = plt.subplots(\n",
    "    nrows=1, ncols=2, sharex=True, sharey=False, figsize=(15,3), squeeze=False)\n",
    "for d in range(D):\n",
    "    subplot_grid[0,0].plot(history['iter'], np.vstack(history['x_D'])[:,d], label='x[%d]' % d);\n",
    "subplot_grid[0,0].set_xlabel('iters')\n",
    "subplot_grid[0,0].set_ylabel('x_d')\n",
    "subplot_grid[0,0].legend(loc='upper right')\n",
    "\n",
    "subplot_grid[0,1].plot(history['iter'], history['f'])\n",
    "subplot_grid[0,1].set_xlabel('iters')\n",
    "subplot_grid[0,1].set_ylabel('f(x)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Solving linear regression with gradient descent + autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe $N$ examples $(x_n, y_n)$ consisting of D-dimensional 'input' vectors $x_n$ and scalar outputs $y_n$.\n",
    "\n",
    "Consider the multivariate linear regression model for making a prediction given any input vector $x_i \\in \\mathbb{R}^D$:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y}(x_i) = w^T x_i\n",
    "\\end{align}\n",
    "\n",
    "One way to train weights would be to just compute the weights that minimize mean squared error\n",
    "\n",
    "\\begin{align}\n",
    "\\min_{w \\in \\mathbb{R}^D}  \\sum_{n=1}^N (y_n - x_n^T w )^2\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Data for linear regression task\n",
    "\n",
    "We'll generate data that comes from an idealized linear regression model.\n",
    "\n",
    "Each example has D=2 dimensions for x.\n",
    "\n",
    "* The first dimension is weighted by +4.2.\n",
    "\n",
    "* The second dimension is weighted by -4.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "D = 2\n",
    "sigma = 0.1\n",
    "\n",
    "true_w_D = np.asarray([4.2, -4.2])\n",
    "true_bias = 0.1\n",
    "\n",
    "train_prng = np.random.RandomState(0)\n",
    "x_ND = train_prng.uniform(low=-5, high=5, size=(N,D))\n",
    "y_N = np.dot(x_ND, true_w_D) + true_bias + sigma * train_prng.randn(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Data Visualization: Pairplots for all possible (x_d, y) combinations\n",
    "\n",
    "You can clearly see the slopes of the lines:\n",
    "* x1 vs y plot: slope is around +4\n",
    "* x2 vs y plot: slope is around -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    data=pd.DataFrame(np.hstack([x_ND, y_N[:,np.newaxis]]), columns=['x1', 'x2', 'y']));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimization problem as an AUTOGRAD-able function wrt the weights w_D\n",
    "def calc_squared_error_loss(w_D):\n",
    "    return ag_np.sum(ag_np.square(ag_np.dot(x_ND, w_D) - y_N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the *loss function* at the known \"ideal\" initial point\n",
    "\n",
    "calc_squared_error_loss(true_w_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Createa an all-zero weight array to use as our initial guess\n",
    "\n",
    "init_w_D = np.zeros(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the *loss function* at that all-zero initial point\n",
    "\n",
    "calc_squared_error_loss(init_w_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use autograd.grad to build the gradient function\n",
    "\n",
    "calc_grad_wrt_w = autograd.grad(calc_squared_error_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the gradient function at that same initial point \n",
    "\n",
    "calc_grad_wrt_w(init_w_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run gradient descent\n",
    "\n",
    "Now let's run GD on our simple regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the gradient's magnitude is very large, use very small step size\n",
    "opt_w_D, history = run_many_iters_of_gradient_descent(\n",
    "    calc_loss, calc_grad_wrt_w, init_w_D,\n",
    "    n_iters=400, step_size=0.00001,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinReg worked example\n",
    "# Make plots of how w_D parameter values evolve over iterations, and function values evolve over iterations\n",
    "# Expected result: x\n",
    "\n",
    "fig_h, subplot_grid = plt.subplots(\n",
    "    nrows=1, ncols=2, sharex=True, sharey=False, figsize=(15,3), squeeze=False)\n",
    "for d in range(D):\n",
    "    subplot_grid[0,0].plot(history['iter'], np.vstack(history['x_D'])[:,d], label='w[%d]' % d);\n",
    "subplot_grid[0,0].set_xlabel('iters')\n",
    "subplot_grid[0,0].set_ylabel('w_d')\n",
    "subplot_grid[0,0].legend(loc='upper right')\n",
    "\n",
    "subplot_grid[0,1].plot(history['iter'], history['f'])\n",
    "subplot_grid[0,1].set_xlabel('iters')\n",
    "subplot_grid[0,1].set_ylabel('-1 * log p(y | w, x)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Autograd for functions of data structures of arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful Fact: autograd can take derivatives with respect to DATA STRUCTURES of parameters\n",
    "\n",
    "This can help us when it is natural to define models in terms of several parts (e.g. NN layers).\n",
    "\n",
    "We don't need to turn our many model parameters into one giant weights-and-biases vector. We can express our thoughts more naturally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 1: gradient of a LIST of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w_list_of_arr):\n",
    "    return ag_np.sum(ag_np.square(w_list_of_arr[0])) + ag_np.sum(ag_np.square(w_list_of_arr[1]))\n",
    "\n",
    "g = autograd.grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_list_of_arr = [np.zeros(3), np.arange(5, dtype=np.float64)]\n",
    "\n",
    "print(\"Type of the gradient is: \")\n",
    "print(type(g(w_list_of_arr)))\n",
    "\n",
    "print(\"Result of the gradient is: \")\n",
    "g(w_list_of_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 2: gradient of DICT of parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(dict_of_arr):\n",
    "    return ag_np.sum(ag_np.square(dict_of_arr['weights'])) + ag_np.sum(ag_np.square(dict_of_arr['bias']))\n",
    "g = autograd.grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_arr = dict(weights=np.arange(5, dtype=np.float64), bias=4.2)\n",
    "\n",
    "print(\"Type of the gradient is: \")\n",
    "print(type(g(dict_of_arr)))\n",
    "\n",
    "print(\"Result of the gradient is: \")\n",
    "g(dict_of_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
